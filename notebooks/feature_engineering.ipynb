{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7eb4da7b",
   "metadata": {},
   "source": [
    "\n",
    "#Feature Engineering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1af8fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import logging\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Add the 'scripts' directory to the Python path for module imports\n",
    "sys.path.append(os.path.abspath(os.path.join('..', 'scripts')))\n",
    "\n",
    "# Import the load_data module\n",
    "try:\n",
    "    from data_loader import load_data\n",
    "    logger_initialized = True\n",
    "except ImportError as e:\n",
    "    logger_initialized = False\n",
    "    print(f\"Error importing 'load_data': {e}\")\n",
    "\n",
    "# Set pandas display options for better visibility\n",
    "pd.set_option('display.max_columns', 200)\n",
    "pd.set_option('display.max_rows', 200)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b78981c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-02 08:55:51,587 - INFO - Imported necessary libraries.\n",
      "2025-07-02 08:55:51,589 - INFO - 'load_data' module imported successfully.\n"
     ]
    }
   ],
   "source": [
    "# Configure logging\n",
    "def setup_logger(name: str = 'my_logger') -> logging.Logger:\n",
    "    \"\"\"\n",
    "    Set up a logger with INFO level and StreamHandler.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    name : str\n",
    "        The name of the logger.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    logging.Logger\n",
    "        Configured logger instance.\n",
    "    \"\"\"\n",
    "    logger = logging.getLogger(name)\n",
    "    logger.setLevel(logging.INFO)\n",
    "    \n",
    "    # Prevent duplicate handlers\n",
    "    if not logger.hasHandlers():\n",
    "        handler = logging.StreamHandler()\n",
    "        handler.setLevel(logging.INFO)\n",
    "        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "        handler.setFormatter(formatter)\n",
    "        logger.addHandler(handler)\n",
    "    \n",
    "    return logger\n",
    "\n",
    "# Initialize logger\n",
    "logger = setup_logger()\n",
    "logger.info(\"Imported necessary libraries.\")\n",
    "\n",
    "# Check and log if 'load_data' was successfully imported\n",
    "if logger_initialized:\n",
    "    logger.info(\"'load_data' module imported successfully.\")\n",
    "else:\n",
    "    logger.warning(\"'load_data' module could not be imported. Check the 'scripts' directory and file availability.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ade522e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-02 08:56:31,628 - INFO - üü¢ Starting the data loading process...\n",
      "2025-07-02 08:56:31,905 - INFO - ‚úÖ Data loaded successfully! The dataset contains 95662 rows and 15 columns.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully loaded from '../data/data.csv' with 95662 rows and 15 columns.\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"üü¢ Starting the data loading process...\")\n",
    "df = load_data('../data/data.csv')\n",
    "if not df.empty:\n",
    "    logger.info(f\"‚úÖ Data loaded successfully! The dataset contains {df.shape[0]} rows and {df.shape[1]} columns.\")\n",
    "else:\n",
    "    logger.warning(\"‚ö†Ô∏è Data loading completed, but the dataset is empty.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1b21f6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the python class for feature engineering\n",
    "from feature_engineering import FeatureEngineering\n",
    "\n",
    "# Instantiate the FeatureEngineering class\n",
    "feature_engineer = FeatureEngineering()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "95da998e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify columns to exclude and categorical columns to encode\n",
    "cols_to_drop = ['ProductId', 'BatchId', 'AccountId', 'ProviderId', 'SubscriptionId', 'Value', 'CountryCode', 'CurrencyCode']\n",
    "cat_features = ['ProductCategory', 'ChannelId']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "67836d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=cols_to_drop, inplace=True, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7b56f331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üü¢ Starting feature engineering process...\n",
      "===============================================\n",
      "Aggregate features created.\n",
      "===============================================\n",
      "Time features extracted.\n",
      "===============================================\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    print(\"üü¢ Starting feature engineering process...\")\n",
    "    print(\"===============================================\")\n",
    "\n",
    "    # Create aggregate features\n",
    "    df_copy = df.copy().reset_index()\n",
    "    agg_features = feature_engineer.create_aggregate_features(df_copy)\n",
    "    print(\"Aggregate features created.\")\n",
    "    print(\"===============================================\")\n",
    "    # Extract time features\n",
    "    df_with_time_features = feature_engineer.extract_time_features(agg_features)\n",
    "    print(\"Time features extracted.\")\n",
    "    print(\"===============================================\")\n",
    "    # Encode categorical features\n",
    "    # df_encoded = feature_engineer.encode_categorical_features(df_with_time_features, cat_features)\n",
    "    # print(\"Categorical features encoded.\")\n",
    "    # print(\"===============================================\")\n",
    "    # Handle missing values\n",
    "    # df_cleaned = feature_engineer.handle_missing_values(df_encoded)\n",
    "    # print(\"Missing values handled.\")\n",
    "    # print(\"===============================================\")\n",
    "    # # Normalize numerical features\n",
    "    # numeric_cols = df_encoded.select_dtypes(include='number').columns\n",
    "    # exclude_cols = ['Amount', 'FraudResult']  # Replace with actual column names to exclude\n",
    "    # numeric_cols = numeric_cols.difference(exclude_cols)\n",
    "\n",
    "    # df_normalized = feature_engineer.normalize_numerical_features(df_encoded, numeric_cols, method='standardize')\n",
    "    # print(\"‚úÖ Numerical features normalized.\")\n",
    "    # print(\"===============================================\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
